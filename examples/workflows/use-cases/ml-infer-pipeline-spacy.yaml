apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: ml-infer-pipeline-spacy-
  namespace: argo
spec:
  entrypoint: ml-infer-pipeline-spacy
  serviceAccountName: hera
  templates:
  - name: ml-infer-pipeline-spacy
    steps:
    - - name: data-prep
        template: data-prep
    - - name: inference-spacy
        template: inference-spacy
  - name: data-prep
    script:
      command:
      - python
      image: jupyter/datascience-notebook:latest
      resources:
        requests:
          cpu: '0.5'
          memory: 1Gi
      source: "import os\nimport sys\nsys.path.append(os.getcwd())\nimport json\n\
        import subprocess\nfrom spacy.lang.en.examples import sentences\nprint(subprocess.run('cd\
        \ /mnt/data && ls -l', shell=True, capture_output=True).stdout.decode())\n\
        subprocess.run(['pip', 'install', 'spacy'], stdout=subprocess.PIPE, universal_newlines=True)\n\
        with open('/mnt/data/input_data.json', 'w') as json_file:\n    json.dump(sentences,\
        \ json_file)\nprint('Data preparation completed')\nprint(subprocess.run('cd\
        \ /mnt/data && ls -l', shell=True, capture_output=True).stdout.decode())"
      volumeMounts:
      - mountPath: /mnt/data
        name: data-dir
  - name: inference-spacy
    script:
      command:
      - python
      image: jupyter/datascience-notebook:latest
      resources:
        requests:
          cpu: '0.5'
          memory: 1Gi
      source: "import os\nimport sys\nsys.path.append(os.getcwd())\nimport subprocess\n\
        subprocess.run(['pip', 'install', 'spacy'], stdout=subprocess.PIPE, universal_newlines=True)\n\
        print(subprocess.run('cd /mnt/data && ls -l ', shell=True, capture_output=True).stdout.decode())\n\
        import json\nfrom typing import List\nimport pydantic\nimport spacy\nfrom\
        \ pydantic import BaseModel\nfrom spacy.cli import download\nspacy_model_name\
        \ = 'en_core_web_lg'\ndownload(spacy_model_name)\nnlp = spacy.load(spacy_model_name)\n\
        print(pydantic.version.version_info())\n\nclass NEROutput(BaseModel):\n  \
        \  input_text: str\n    ner_entities: List[str] = []\nner_output_list: List[NEROutput]\
        \ = []\nwith open('/mnt/data/input_data.json', 'r') as json_file:\n    input_data\
        \ = json.load(json_file)\n    print(input_data)\n    for sentence in input_data:\n\
        \        print('input text: ' + sentence)\n        doc = nlp(sentence)\n \
        \       print('output NER:')\n        ner_entities: List[str] = []\n     \
        \   for entity in doc.ents:\n            ner_entity = entity.text + ' is '\
        \ + entity.label_\n            print(ner_entity)\n            ner_entities.append(ner_entity)\n\
        \        print('ner_entities = + ' + ner_entities)\n        ner_output = NEROutput(input_text=sentence,\
        \ ner_entities=ner_entities)\n        ner_output_list.append(dict(ner_output))\n\
        \    print('ner_output_list = ' + ner_output_list)\nprint('Inference completed')\n\
        with open('/mnt/data/output_data.json', 'w') as json_file:\n    json.dump(ner_output_list,\
        \ json_file)\nprint(subprocess.run('cd /mnt/data && ls -l ', shell=True, capture_output=True).stdout.decode())"
      volumeMounts:
      - mountPath: /mnt/data
        name: data-dir
  volumeClaimTemplates:
  - metadata:
      name: data-dir
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
