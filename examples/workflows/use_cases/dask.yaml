apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: dask-
spec:
  entrypoint: s
  templates:
  - name: s
    steps:
    - - name: dask-computation
        template: dask-computation
  - name: dask-computation
    inputs:
      parameters:
      - name: namespace
        default: default
      - name: n_workers
        default: '1'
    script:
      image: ghcr.io/dask/dask:latest
      source: |-
        import os
        import sys
        sys.path.append(os.getcwd())
        import json
        try: n_workers = json.loads(r'''{{inputs.parameters.n_workers}}''')
        except: n_workers = r'''{{inputs.parameters.n_workers}}'''
        try: namespace = json.loads(r'''{{inputs.parameters.namespace}}''')
        except: namespace = r'''{{inputs.parameters.namespace}}'''

        import subprocess
        subprocess.run(['pip', 'install', 'dask-kubernetes', 'dask[distributed]'], stdout=subprocess.PIPE, universal_newlines=True)
        import dask.array as da
        from dask.distributed import Client
        from dask_kubernetes.operator import KubeCluster
        cluster = KubeCluster(image='ghcr.io/dask/dask:latest', resources={'requests': {'memory': '2G', 'cpu': '1'}, 'limits': {'memory': '4G', 'cpu': '1'}}, namespace=namespace, n_workers=n_workers)
        client = Client(cluster)
        array = da.ones((1000, 1000, 1000))
        print('Array mean = {array_mean}, expected = 1.0'.format(array_mean=array.mean().compute()))
        client.close()
      command:
      - python
