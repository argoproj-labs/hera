apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: dask-
spec:
  entrypoint: s
  templates:
  - name: s
    steps:
    - - name: dask-computation
        template: dask-computation
  - inputs:
      parameters:
      - default: default
        name: namespace
      - default: '1'
        name: n_workers
    name: dask-computation
    script:
      command:
      - python
      image: ghcr.io/dask/dask:latest
      source: |-
        import os
        import sys
        sys.path.append(os.getcwd())
        import json
        try: n_workers = json.loads(r'''{{inputs.parameters.n_workers}}''')
        except: n_workers = r'''{{inputs.parameters.n_workers}}'''
        try: namespace = json.loads(r'''{{inputs.parameters.namespace}}''')
        except: namespace = r'''{{inputs.parameters.namespace}}'''

        import subprocess
        subprocess.run(['pip', 'install', 'dask-kubernetes', 'dask[distributed]'], stdout=subprocess.PIPE, universal_newlines=True)
        import dask.array as da
        from dask.distributed import Client
        from dask_kubernetes.operator import KubeCluster
        cluster = KubeCluster(image='ghcr.io/dask/dask:latest', resources={"requests": {"memory": "2G", "cpu": "1"}, "limits": {"memory": "4G", "cpu": "1"}}, namespace=namespace, n_workers=n_workers)
        client = Client(cluster)
        array = da.ones((1000, 1000, 1000))
        print('Array mean = {array_mean}, expected = 1.0'.format(array_mean=array.mean().compute()))
        client.close()
